{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "# from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:,i] = (X[:,i] - X[:,i].mean())/X[:,i].std()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "p = 8\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Returns the data from the xlsx file\n",
    "    \"\"\"\n",
    "    file_name = 'yeast.csv' \n",
    "    df = pd.read_csv(file_name, index_col=0, header=None)\n",
    "    # print(df.head()) # print the first 5 rows\n",
    "    return df\n",
    "\n",
    "md = get_data()\n",
    "md.dropna(inplace = True)\n",
    "print(md.shape)\n",
    "md.replace('?', 0, inplace = True)\n",
    "\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = md.iloc[:, 0:8].values.reshape(-1, 8)\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "# add 1 to the first column\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "# np.random.shuffle(X)\n",
    "\n",
    "print('X:', X)\n",
    "print(X.shape)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# print(preprocessing.scale(X))\n",
    "X = normalize(X)\n",
    "print(X)\n",
    "# T = preprocessing.scale(X)\n",
    "\n",
    "M = X.shape[0]\n",
    "Y = md.iloc[:, 8:9].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(i):\n",
    "    return 1 / (1 + np.exp(-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['CYT', 'NUC', 'MIT', 'ME3', 'ME2', 'ME1', 'EXC', 'VAC', 'POX', 'ERL']\n",
    "\n",
    "y = np.zeros((len(Y), 10))\n",
    "print(y.shape)\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    for j in range(10):\n",
    "        if outputs[j] == Y[i]:\n",
    "            y[i][j] = 1\n",
    "        else:\n",
    "            y[i][j] = 0\n",
    "\n",
    "print('results', Y[0] == outputs[2])\n",
    "print(y.shape)\n",
    "# for i in range(10):\n",
    "#     print(outputs[i], ': ', y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "beta = np.zeros((10, 8))\n",
    "\n",
    "for i in range(10):\n",
    "    beta[i] = np.random.rand(1, 8)\n",
    "\n",
    "print(beta)\n",
    "gradient = np.zeros((10, 8))\n",
    "trainNo = 1000\n",
    "loopCount = 1000\n",
    "\n",
    "n = 0\n",
    "while (True):\n",
    "    for i in range(trainNo):   \n",
    "        tmp = p(X[i] @ beta.T)\n",
    "        tmp = tmp - y[i]\n",
    "        for t in range(k):\n",
    "            gradient[t] = gradient[t] + tmp[t] * X[i]\n",
    "    gradient = gradient / trainNo\n",
    "    \n",
    "    \n",
    "    beta = beta - eta * gradient\n",
    "    print('g: ', gradient, 'norm', np.linalg.norm(gradient))\n",
    "\n",
    "    n = n + 1\n",
    "    if n == trainNo or np.linalg.norm(gradient) < 0.0097:\n",
    "        break\n",
    "    eta = eta * 0.99\n",
    "\n",
    "print('beta: ', beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta = 0.001\n",
    "# trainNo = 1200\n",
    "# beta = np.zeros((10, 9))\n",
    "# for t in range(10):\n",
    "#     for i in range(trainNo):\n",
    "#         # take one sample randomly\n",
    "#         # i = np.random.randint(0, trainNo)\n",
    "#         print(i)\n",
    "        \n",
    "#         # calculate the gradient\n",
    "#         for k in range(10):\n",
    "#             for j in range(9):\n",
    "#                 gradient[k][j] = (p(np.dot(X[i], beta[k])) - y[i][k]) * X[i][j]\n",
    "\n",
    "#         print('grad', gradient)\n",
    "\n",
    "#         # update beta\n",
    "#     for k in range (10):\n",
    "#         beta[k] = beta[k] - eta * gradient[k] / trainNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 1. 2. 1. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 0. 6. 2. 2. 1. 0. 0. 1.\n",
      " 2. 2. 1. 1. 1. 0. 1. 1. 2. 3. 1. 1. 6. 0. 1. 2. 0. 3. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 4. 3. 0. 0. 0. 0. 0. 3. 9. 2. 3. 3. 2. 1. 3. 0. 3. 3. 3. 0. 3. 3.\n",
      " 0. 1. 1. 0. 6. 9. 3. 1. 3. 1. 0. 2. 6. 4. 3. 2. 3. 3. 2. 1. 1. 0. 2. 2.\n",
      " 1. 1. 1. 0. 1. 3. 3. 0. 1. 1. 1. 2. 3. 0. 1. 3. 1. 3. 1. 3. 3. 0. 2. 3.\n",
      " 3. 2. 1. 3. 3. 1. 1. 0. 0. 0. 3. 0. 2. 1. 3. 1. 0. 0. 0. 1. 3. 1. 2. 2.\n",
      " 3. 8. 1. 3. 3. 1. 0. 3. 3. 0. 1. 1. 1. 0. 0. 3. 0. 0. 0. 1. 2. 1. 3. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 1. 1. 3. 3. 2. 3. 3. 6. 1. 3. 3.\n",
      " 1. 3. 1. 4. 1. 1. 3. 2. 1. 1. 3. 1. 3. 3. 3. 6. 6. 6. 0. 0. 0. 1. 3. 0.\n",
      " 1. 3. 2. 2. 3. 0. 1. 1. 0. 3. 3. 6. 3. 3. 1. 3. 0. 0. 4. 3. 0. 2. 2. 2.\n",
      " 1. 0. 0. 0. 1. 1. 2. 1. 1. 1. 1. 1. 1. 0. 2. 0. 3. 0. 3. 1. 1. 1. 1. 0.\n",
      " 0. 4. 0. 4. 6. 0. 0. 0. 1. 0. 1. 1. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 2. 0. 3. 1. 2. 4. 0. 0. 0. 0. 2. 1. 1. 0. 1. 0. 0. 0.\n",
      " 3. 0. 0. 1. 0. 1. 1. 1. 3. 1. 1. 2. 0. 0. 0. 2. 1. 3. 0. 0. 0. 1. 0. 3.\n",
      " 0. 3. 0. 0. 0. 0. 1. 3. 3. 1. 4. 0. 0. 0. 2. 0. 0. 0. 3. 1. 4. 1. 3. 1.\n",
      " 3. 3. 3. 3. 4. 3. 1. 2. 2. 2. 3. 3. 3. 2. 2. 3. 1. 3. 0. 2. 3. 3. 1. 3.\n",
      " 1. 3. 2. 3. 8. 8. 3. 6. 3. 3. 3. 1. 2. 2. 3. 3. 3. 3. 3. 3. 0. 3. 3. 3.\n",
      " 3. 1. 4. 3. 1. 3. 0. 1. 3. 3. 3. 3. 4. 3. 1. 3. 3. 1. 3. 3. 3. 2. 3. 3.\n",
      " 4. 3. 4. 1. 6. 6. 3. 2. 3. 1. 4. 3. 3. 3. 2. 1. 1. 6. 3. 0. 1. 3. 2. 1.\n",
      " 4. 0. 3. 1. 2. 2. 2. 1. 1. 1. 3. 0. 3. 3. 2. 0. 1. 0. 0. 4. 0. 2. 0. 4.\n",
      " 1. 3. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.5268595041322314 484\n"
     ]
    }
   ],
   "source": [
    "# one or not classification\n",
    "x_pred = np.zeros(trainNo)\n",
    "estimates = np.zeros(10)\n",
    "\n",
    "for i in range(M - trainNo):\n",
    "    for t in range(10):\n",
    "        estimates[t] = p(np.dot(X[trainNo + i], beta[t]))\n",
    "    x_pred[i] = np.argmax(estimates)\n",
    "    # print(estimates)\n",
    "    # print(x_pred[i])\n",
    "print(x_pred)\n",
    "\n",
    "correct = 0\n",
    "for i in range(M - trainNo):\n",
    "    if np.where(y[trainNo + i] == 1) == x_pred[i]:\n",
    "        correct = correct + 1\n",
    "print(correct / (M - trainNo), (M - trainNo))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ec9399770a17bbf0be3187b8c09b2d3c81bf49ccab5407e3aee235c2dd27d7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
